{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- attention mechanism me same **encoder and decoder** hota hai aur in dono me stacked lstm hi use hota hai. \n",
    "- encoder and decoder q fail hua:\n",
    "    - q ki ye lambe sequences pe achcha result nhi de paata hai jaise yadi is ko greater than 25 or 30 words se jyada wala sequence diya to ye fail kr jata hai. aur phir isko overcome krne ke liye attention mechanism aata hai.\n",
    "- What is attention mechansim?\n",
    "    - attention mechanism har ek current timestamp ye batata hai ki kaun sa word jyada important hai output dene ke liye or translate krne ke liye. \n",
    "- BLUE score se machine translation ka score nikalte hain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
